---
layout: posts
title: "Data Structures and Algorithms : Big-O Notation"
author: Kane Norman
categories: [programming]
excerpt: How to measure efficiency and scalability
---

<div class="discussion-content">
  <h4>Introduction</h4>
  <p>
    As a computer scientist, predicting the exact amount of time an algorithm
    will take to execute can be extremely difficult. Various factors can affect
    the execution time, and even the same device may not always produce
    consistent results. Instead of focusing on the exact execution time, we are
    often more interested in understanding how an algorithm performs as the
    input size increases. For instance, will an algorithm that sorts an array of
    10 elements take longer to sort an array of 1,000,000 elements? If so, can
    we quantify how much longer it will take? This is where Big-O notation comes
    into play. Big-O notation provides a way to describe an algorithm's expected
    time complexity in terms of the number or magnitude of it inputs.
  </p>

  <p>
    In this post, we will explore the importance of Big-O notation in analyzing
    algorithm performance. While the mathematical theory behind Big-O notation
    can be complex, I will focus on providing simple, easy-to-understand
    examples to help you comprehend the basics of Big-O notation. By using
    real-world scenarios, I will demonstrate how Big-O notation can help you
    compare the efficiency of different algorithms and make informed decisions
    when implementing them. By the end of this post, you should have a solid
    understanding of Big-O notation and its significance in computer science.
  </p>

  <hr />
  <h4>Best, Worst, and Average Case</h4>
  <p>
    When analyzing algorithms, we may encounter inputs that meet special
    conditions that can significantly affect their performance. Some inputs
    create optimal performance conditions, allowing for far better than average
    runtime. We refer to this as the best-case time complexity, denoted by the Ω
    symbol. The average case represents the performance we can expect on average
    when the input does not meet any exceptional conditions, and it is denoted
    by the θ symbol. Finally, the worst-case time complexity refers to the
    performance when the input meets some condition that causes the algorithm to
    perform its absolute worst. This is denoted by the capital O symbol.
  </p>

  <p>
    While it's important to consider all three cases, in practice, we usually
    focus on the average and worst cases. For most algorithms, these cases tend
    to be the same. In this post, we'll primarily focus on the worst-case time
    complexity, since it provides an upper bound on the amount of time an
    algorithm will take to complete.
  </p>

  <hr />
  <h4>Rules for Big O</h4>
  There are a four main rules to follow for big-O notation:
  <ul>
    <li>Drop non dominant terms</li>
    <li>Different steps can be summed of multiplied</li>
  </ul>

  <hr />
  <h4>O(1)</h4>
  <p>
    O(1) represents constant time complexity. This means that the time required
    by the algorithm does not change with the size of the input. Instead, the
    code executes at a constant rate. For instance, consider the following code
    snippet:
  </p>
  <pre>
  {%highlight python%}
def get_first_element(array):
    return array[0]
  {%endhighlight%}
</pre
  >
  <p>
    The function above, <code>get_first_element()</code>, is a straightforward
    operation that retrieves the first element of an input array. It does so by
    accessing the base memory address of the array and returning the value
    stored at that address. Since it performs the same number of operations
    regardless of the size of the input array, this function runs in constant
    time. This means that whether the array has one element or one million
    elements, we can expect <code>get_first_element()</code> to execute in the
    same amount of time.
  </p>

  <hr />
  <h4>O(n)</h4>
  <p>
    O(n) represents linear time complexity, which means that the time taken by
    an algorithm increases in direct proportion to the size of the input. In
    other words, if the input size increases by a factor of k, then the time
    taken by the algorithm will also increase by a factor of k. For example, if
    an algorithm takes 5 seconds to process an array with 10 elements, it would
    take around 50 seconds to process an array with 100 elements and around 500
    seconds to process an array with 1,000 elements.
  </p>
  <p>
    A simple example to illustrate linear time complexity is the following
    function:
  </p>
  <pre>
    {% highlight python %}
def sum_array(array):
    total = 0
    for number in array:
        total += number
    return total
    {% endhighlight %}
</pre
  >
  <p>
    In this example, the number of operations performed is directly proportional
    to the size of the input array. If the array contains n elements, n
    additions will need to be performed. The graph below shows the performance
    of the function <code>sum_array()</code> as the size of the input increases.
    As expected, the relationship between input size and average execution time
    is clearly linear.
  </p>
  <figure>
    <iframe
      width="100%"
      height="500"
      frameborder="0"
      seamless="seamless"
      scrolling="no"
      src="/charts_and_graphs/On_example.html"
    ></iframe>
  </figure>

  <p>
    Another example of a linear algorithm is the linear search algorithm, which
    iterates through an array until it finds the value it is looking for. Here's
    an implementation of the algorithm in Python:
  </p>
  <pre>
    {% highlight python %}
def linear_search(array, value):
    for i in range(len(array)):
        if array[i] == value:
            return i
    return -1
    {% endhighlight %}
</pre
  >
  <p>
    In the worst-case scenario for a linear search algorithm, where the target
    value is located at the end of the array, the algorithm would need to
    perform n iterations to find it, where n is the number of elements in the
    array. Therefore, the worst-case time complexity of the linear search
    algorithm is O(n), where n is the input size.
  </p>
  <p>
    Here's a tricky question: If we add an extra constant operation to the
    function <code>sum_array()</code> like the code snippet below, would the
    time complexity still be considered O(n), or would it be considered O(n+1)?
  </p>
  <pre>
    {% highlight python %}
def sum_array(array):
    total = 0
    for number in array:
        total += number

    # adding a constant operation
    total = total + 1 
    return total 
    {% endhighlight %}
</pre
  >
  <p>
    It would still be considered O(n) time complexity. The reason why is because
    we drop constants when calculating big-O notation. Remember, the purpose of
    big-O is to measure how the algorithm scales as the input size increases,
    and we are only concerned with the general trend of the algorithm's
    behavior. In other words, we don't care about the exact amount of time the
    algorithm takes to execute, but rather about how that time changes as the
    input size grows.
  </p>
  <p>
    To put it in mathematical terms, we don't care if the time a function takes
    to execute is modeled by the function f(x) = x, f(x) = ax, f(x) = x + b, or
    f(x) = ax + b. All we care about is that the function is a linear function
    that can be described proportionally to x.
  </p>

  <hr />
  <h4>O(n<sup>2</sup>)</h4>
  <p>
    O(n<sup>2</sup>) represents quadratic time complexity, meaning that the
    algorithm scales proportional to the square of the size of the input. In
    other words, if the input size increases by a factor of k, then the time
    taken by the algorithm will increase by a factor of k<sup>2</sup>. For
    example, if an algorithm takes 100 second to process an array with 10
    elements, it would take around 10,000 seconds to process an array with 100
    elements and around 1,000,000 seconds to process an array with 1,000
    elements. A common example of an algorithm with quadratic time complexity is
    the nested loop, where a loop runs inside another loop. An example of such
    an algorithm is the following function that finds all the pairs of numbers
    in an array that add up to a target sum:
  </p>
  <pre>
    {% highlight python %}
def find_pairs_with_sum(array, target_sum):
    pairs = []
    for i in range(len(array)):
        for j in range(i+1, len(array)):
            if array[i] + array[j] == target_sum:
                pairs.append((array[i], array[j]))
    return pairs
    {% endhighlight %}
</pre
  >
  <p>
    In this function, for each element in the array, it checks every other
    element to see if their sum is equal to the target sum. This results in a
    nested loop where the number of iterations is dependent on the size of the
    array. The number of operations performed by this algorithm is proportional
    to the square of the input size, making its time complexity
    O(n<sup>2</sup>). As the input size grows, the time taken by the algorithm
    increases rapidly. The graph below shows the performance of the function
    <code>find_pairs_with_sum()</code> as the size of the input array increases.
    As expected, the relationship between input size and time is clearly
    quadratic.
  </p>
  <iframe
    width="100%"
    height="500"
    frameborder="0"
    seamless="seamless"
    scrolling="no"
    src="/charts_and_graphs/On2_example.html"
  ></iframe>
  <hr />
  <h4>O(log n)</h4>
  <p>
    O(log n) is a measure of logarithmic time complexity, indicating that the
    time taken by an algorithm grows in proportion to the logarithm of the input
    size. As the input size increases, the time taken by the algorithm will
    increase much slower than linearly, making it efficient for large inputs.
    For instance, if an algorithm takes 10 seconds to process an input of size
    1,000, it would take only around 20 seconds to process an input of size
    1,000,000, which is a significantly smaller increase in time compared to the
    linear case.
  </p>
  <p>
    A popular example of an algorithm with O(log n) complexity is binary search.
    It repeatedly divides a sorted array in half until the target value is
    found. As the size of the array doubles, the number of steps needed to find
    the target value increases only by one, making binary search an extremely
    efficient algorithm for large inputs. An implementation is provided below,
    but don't worry too much about understanding the code just yet. It will be
    discussed in detail in a future post.
  </p>
  <pre>
      {% highlight python %}
def binary_search(array, value):

    low = 0
    high = len(array) - 1

    while low <= high:
        middle = (low + high) // 2

        if array[middle] == value:
            return middle

        # if midpoint is smaller, ignore the left half
        if array[middle] < value:
            low = middle + 1

        # if midpoint is larger, ignore the right half
        if array[middle] > value:
            high = middle - 1

    # return -1 if match not found
    return -1
      {% endhighlight %}
  </pre>
  <p>
    The graph below shows the performance of our O(log n)
    <code>binary_search()</code> algorithm compared to O(n)
    <code>linear search()</code>
    as the size of our input array increases increases. As expected, the O(log
    n) algorithm has a much shallower curve, indicating slower growth in time as
    the input size increases.
  </p>
  <iframe
    width="100%"
    height="500"
    frameborder="0"
    seamless="seamless"
    scrolling="no"
    src="/charts_and_graphs/binary_vs_linear_search.html"
  ></iframe>

  <p>
    For those who are interested in a mathematical explanation of the log
    function, recall that if log<sub>b</sub>(n) roughly tells us how many times
    we can subdivide n by b. For example, if we wanted to know how many times we
    can divide 100 by 2, we simply calculate log<sub>2</sub>(100), which equals
    6.44 (approximately). This means that binary search would subdivide an array
    of size 100 into at most 7 parts to find a maximum value, making it a very
    efficient algorithm for large inputs.
  </p>
</div>
