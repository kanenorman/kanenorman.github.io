---
layout: posts
title: "Data Structures and Algorithms : Big-O Notation"
author: Kane Norman
categories: [programming]
excerpt: How to measure efficiency and scalability
---

<div class="discussion-content">
  <h4>Introduction</h4>

  <p>
    In the
    <a href="{{ '/programming/data-structures-and-algorithms' | relative_url }}"
      >previous post</a
    >
    , I mentioned that it is difficult to predict the
    <span style="font-style: italic; font-weight: bold; color: darkorange"
      >exact</span
    >
    amount of time an algorithm takes to execute. Since it is difficult to
    predict the exact execution time, we often focus on understanding how the
    execution time scales as the size of the input(s) increases. For instance,
    will an algorithm that sorts 10 elements take longer to sort 1,000,000
    elements? If so, can we quantify how much longer it will take? This is where
    Big-O notation comes into play. Big-O notation provides a way to describe an
    algorithm's expected execution time in terms of the number or magnitude of
    it inputs (n).
  </p>
  <p>
    The graph presented below illustrates several common Big-O notations. It
    shows that certain complexities, like O(log n), require a relatively small
    number of operations, even as the number of elements grows. In contrast, for
    other complexities like O(n!), the number of required operations increases
    rapidly as the size of the input increases. This rapid increase in
    operations can cause poor performance if we handle large amounts of data.
    For this reason, we need to be aware of situations where Big-O tells us that
    we may encounter a large number of operations leading to poor performance.
  </p>

  <div style="text-align: center">
    <figure>
      <img
        src="{{ '/images/Big-O-graph.png' | relative_path}}"
        alt="Comparison of
      Big-0 time complexities"
        style="max-width: -webkit-fill-available"
      />
      <figcaption>
        Image courtesy of
        <a href="https://www.bigocheatsheet.com">bigocheatsheet.com</a>
      </figcaption>
    </figure>
  </div>

  <p></p>

  <p>
    In this post, we will explore the importance of Big-O notation in analyzing
    algorithm performance. Although the mathematical theory behind Big-O
    notation can be complicated, I will provide simple, easy-to-understand
    examples to help you grasp the basics of Big-O notation. By using real-world
    scenarios, I will demonstrate how Big-O notation can help you compare the
    efficiency of different algorithms and make informed decisions when
    implementing them. By the end of this post, you should have a solid
    understanding of Big-O notation and its significance in computer science.
  </p>

  <hr />
  <h4>Best, Average, and Worst Case</h4>
  <p>
    When analyzing the performance of algorithms, we may encounter input
    arguments that meet special conditions and significantly affect performance.
    Some inputs can create optimal performance conditions, resulting in far
    better runtime than the average case, while others can create worst-case
    conditions, leading to significantly worse performance. Therefore, it is
    important to consider the range of inputs that our algorithm may encounter
    when evaluating its performance. To ensure a fair evaluation, we can
    calculate three different time complexities for an algorithm:
  </p>
  <ul>
    <li>
      <b>Best-case time complexity (Ω):</b> represents the minimum possible
      runtime of an algorithm under ideal conditions. It occurs when the
      algorithm's performance is optimal due to input values that make it run
      faster than usual.
    </li>
    <li>
      <b>Average-case time complexity (θ):</b> represents the performance we can
      expect on average when the input does not meet any exceptional conditions.
    </li>
    <li>
      <b>Worst-case time complexity (O):</b> represents the maximum possible
      runtime of an algorithm under the least favorable conditions. It occurs
      when the algorithm's performance is the worst due to input values that
      make it run slower than usual.
    </li>
  </ul>

  <p>
    Here is an example to demonstrate these three cases. Consider the Python
    List method
    <code>index()</code>, which searches for a target value in a Python List and
    returns the index of its first occurrence. The performance of the method can
    be evaluated based on the best, average, and worst-case scenarios:
  </p>

  <ul>
    <li>
      Best-case scenario: occurs when the element we are looking for is the
      first element in the list. We won't have to search through any other
      values in the list, and the method can return almost immediately,
      resulting in the minimum possible runtime.
    </li>
    <li>
      Average-case scenario: occurs when the value we are searching for is
      located somewhere in the middle of the list. In this scenario, we will
      have to search through about half the elements in the list until we find
      our target value, resulting in an average runtime.
    </li>
    <li>
      Worst-case scenario: occurs when the element is not in the list or occurs
      towards the end. In this case, the method will have to search through a
      large portion of the list, leading to the longest possible runtime.
    </li>
  </ul>

  <pre>
{%highlight python%}
# list that looks like [0, 1, 2, 3, ...97, 98, 99]
numbers = [number for number in range(100)]

# specific elements in array to use as inputs
first = 0
middle = 50
last = 99

# create three scenarios based on inputs
best_case = numbers.index(first)
average_case = numbers.index(middle)
worst_case = numbers.index(last)
{%endhighlight%}
  </pre>
  <p>
    Below we can see the execution times for the <code>best_case</code>,
    <code>average_case</code>, and <code>worst_case</code> scenarios. As
    expected, the best-case scenario executed the fastest, followed by the
    average case and the worst-case, respectively.
  </p>

  <iframe
    width=" 100%"
    height="500"
    frameborder="0"
    seamless="seamless"
    scrolling="no"
    src="/charts_and_graphs/best_average_worst.html"
  ></iframe>

  <p>
    By understanding the performance characteristics of the
    <code>index()</code> method under different scenarios, we can make informed
    decisions about when to use it in our code. Overall, understanding the best,
    average, and worst-case scenarios can help us to optimize our code and avoid
    unexpected performance issues.
  </p>

  <p>
    In this post, we'll primarily focus on the worst-case time complexity, as it
    provides an upper bound on the amount of time that an algorithm will take to
    complete. Understanding the worst-case scenario is particularly important in
    time-critical applications, where we need to guarantee that the algorithm
    will complete within a certain time frame. However, we should keep in mind
    that the worst-case scenario may not always be the most common or
    representative of real-world performance. Therefore, we should also consider
    the average-case scenario to ensure that our algorithm meets performance
    requirements in typical use cases.
  </p>
  <hr />
  <h4>Rules for Big-O Notation</h4>
  The following guidelines should be kept in mind while determining Big-O
  notation:
  <ul>
    <li>
      <b>Ignore non-dominant terms and constants:</b> When analyzing the time
      complexity of an algorithm, we can safely ignore the non-dominant terms
      and constants. This allows us to focus on the most significant factors
      that affects the run time of the algorithm. For example, O(2n + 1) can be
      simplified to O(n), and O(n<sup>2</sup> + n) can be simplified to
      O(n<sup>2</sup>) By doing so, we only focus on the most dominant terms
      that significantly impact runtime performance and provide a general
      expected runtime rather than a specific one.
    </li>
    <li>
      <b>Treat different inputs as separate variables:</b> When analyzing the
      time complexity of an algorithm, it's important to consider the impact of
      multiple inputs. Some algorithms may take two-dimensional inputs, or
      multiple function arguments. It's important to treat these inputs as
      separate variables when determining the Big-O notation.
    </li>
    <li>
      <b>Sum or multiply steps:</b> When an algorithm has multiple steps, we can
      either sum or multiply the complexities of each step to obtain the overall
      time complexity. For example, if an algorithm repeats an O(log n)
      operation n times, the overall time complexity would be O(n) x O(log n) =
      O(n log n).
    </li>
  </ul>
  <p>
    It's important to keep in mind that the purpose of Big-O notation is to
    provide a general understanding of how an algorithm scales. We are not
    concerned with the exact time it takes to execute an algorithm, but rather
    how it behaves as the input size grows
  </p>

  <hr />
  <h4>O(1)</h4>
  <p>
    O(1) represents constant time complexity. This means that the time required
    by the algorithm does not change with the size of the input. Instead, the
    code executes at a constant rate. For instance, consider the following code
    snippet:
  </p>
  <pre>
  {%highlight python%}
def get_first_element(array):
    return array[0]
  {%endhighlight%}
</pre
  >
  <p>
    The function above, <code>get_first_element()</code>, is a straightforward
    operation that retrieves the first element of an input array. It does so by
    accessing the base memory address of the array and returning the value
    stored at that address. Since it performs the same number of operations
    regardless of the size of the input array, this function runs in constant
    time. This means that whether the array has one element or one million
    elements, we can expect <code>get_first_element()</code> to execute in the
    same amount of time.
  </p>

  <p>
    As an important note, earlier that we said we drop constants when reporting
    Big-O. O(1) is the special case that breaks this rule. Any time we have
    constant runtime we report it as O(1).
  </p>

  <hr />
  <h4>O(n)</h4>
  <p>
    O(n) represents linear time complexity, which means that the time taken by
    an algorithm increases in direct proportion to the size of the input. In
    other words, if the input size increases by a factor of k, then the time
    taken by the algorithm will also increase by a factor of k. For example, if
    an algorithm takes 5 seconds to process an array with 10 elements, it would
    take around 50 seconds to process an array with 100 elements and around 500
    seconds to process an array with 1,000 elements.
  </p>
  <p>
    A simple example to illustrate O(n) time complexity is the following
    function:
  </p>
  <pre>
    {% highlight python %}
def sum_array(array):
    total = 0
    for number in array:
        total += number
    return total
    {% endhighlight %}
</pre
  >
  <p>
    In this example, the number of operations performed is directly proportional
    to the size of the input array. If the array contains n elements, n
    additions will need to be performed. The graph below shows the performance
    of the function <code>sum_array()</code> as the size of the input increases.
    As expected, the relationship between input size and average execution time
    is clearly linear.
  </p>
  <figure>
    <iframe
      width="100%"
      height="500"
      frameborder="0"
      seamless="seamless"
      scrolling="no"
      src="/charts_and_graphs/On_example.html"
    ></iframe>
  </figure>

  <p>
    Another example of a O(n) algorithm is the linear search algorithm, which
    iterates through an array until it finds the value it is looking for
    (similar to the Python <code>index()</code> method we discussed earlier).
    Here's an implementation of the algorithm in Python:
  </p>
  <pre>
    {% highlight python %}
def linear_search(array, value):
    for i in range(len(array)):
        if array[i] == value:
            return i
    return -1
    {% endhighlight %}
</pre
  >
  <p>
    In the worst-case scenario for a linear search algorithm, the target value
    is located at the end of the array. This means the algorithm would need to
    perform n iterations to find our target value, where n is the number of
    elements in the array. Therefore, the worst-case time complexity of the
    linear search algorithm is O(n).
  </p>
  <p>
    Here's a tricky question: If we add an extra constant operation to the
    function <code>sum_array()</code> like the code snippet below, would the
    time complexity still be considered O(n), or would it be considered O(n+1)?
  </p>
  <pre>
    {% highlight python %}
def sum_array(array):
    total = 0
    for number in array:
        total += number

    # adding a constant operation
    total = total + 1 
    return total 
    {% endhighlight %}
</pre
  >
  <p>
    It would still be considered O(n) time complexity. The reason why is because
    we drop constants when calculating Big-O notation. Remember, the purpose of
    Big-O is to measure how the algorithm scales as the input size increases,
    and we are only concerned with the general trend of the algorithm's
    behavior. In other words, we don't care about the exact amount of time the
    algorithm takes to execute, but rather about how that time changes as the
    input size grows.
  </p>
  <p>
    To put it in mathematical terms, we don't care if the time a function takes
    to execute is modeled by the function f(x) = x, f(x) = ax, f(x) = x + b, or
    f(x) = ax + b. All we care about is that the function is a linear function
    that can be described proportionally to x.
  </p>

  <hr />
  <h4>O(n<sup>2</sup>)</h4>
  <p>
    O(n<sup>2</sup>) represents quadratic time complexity, meaning that the
    algorithm scales proportional to the square of the size of the input. In
    other words, if the input size increases by a factor of k, then the time
    taken by the algorithm will increase by a factor of k<sup>2</sup>. For
    example, if an algorithm takes 100 second to process an array with 10
    elements, it would take around 10,000 seconds to process an array with 100
    elements and around 1,000,000 seconds to process an array with 1,000
    elements.
  </p>
  <p>
    A common example of an algorithm with quadratic time complexity is the
    nested loop, where a loop runs inside another loop. An example of such an
    algorithm is the following function that finds all the pairs of numbers in
    an array that add up to a target sum:
  </p>
  <pre>
    {% highlight python %}
def find_pairs_with_sum(array, target_sum):
    n = len(array)
    pairs = []

    for i in range(n):
        for j in range(i+1, n):
            if array[i] + array[j] == target_sum:
                pairs.append((array[i], array[j]))

    return pairs
    {% endhighlight %}
  </pre>
  <p>
    In this function, for each element in the array, it checks every other
    element to see if their sum is equal to the target sum. We can calculate the
    number of comparisons this algorithm preforms in the table below:
  </p>

  <table class="tg">
    <thead>
      <tr>
        <th>i</th>
        <th>j</th>
        <th>comparisons</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0</td>
        <td>[1,n)</td>
        <td>n-1</td>
      </tr>
      <tr>
        <td>1</td>
        <td>[2,n)</td>
        <td>n-2</td>
      </tr>
      <tr>
        <td>2</td>
        <td>[3,n)</td>
        <td>n-3</td>
      </tr>
      <tr>
        <td>...</td>
        <td>...</td>
        <td>...</td>
      </tr>
      <tr>
        <td>n-2</td>
        <td>[n-1,n)</td>
        <td>1</td>
      </tr>
      <tr>
        <td>n-1</td>
        <td>[n,n)</td>
        <td>0</td>
      </tr>
    </tbody>
  </table>
  <p>
    Thus, the number of comparisons made by this algorithm can be expressed as
    the sum of the series (n-1) + (n-2) + (n-3) + ... + 1 + 0, which is
    equivalent to (n<sup>2</sup>-n)/2. Therefore, using the Big-O notation
    rules, we can say that the number of operations performed by this algorithm
    is proportional to the square of the input size, making its time complexity
    O(n<sup>2</sup>). As the input size grows, the time taken by the algorithm
    increases quadratically. The graph below shows the performance of the
    function
    <code>find_pairs_with_sum()</code> as the size of the input array increases.
    As expected, the relationship between input size and time is clearly
    quadratic.
  </p>
  <iframe
    width="100%"
    height="500"
    frameborder="0"
    seamless="seamless"
    scrolling="no"
    src="/charts_and_graphs/On2_example.html"
  ></iframe>
  <hr />
  <h4>O(mn)</h4>

  <p>
    The notation O(mn) indicates that the time complexity of an algorithm is
    proportional to both m and n. This notation is commonly used to express the
    time complexity of an algorithm with two variables, particularly when
    working with matrices or two-dimensional arrays.
  </p>
  <div style="text-align: center">
    <img
      src="{{ '/images/mxn_matrix.png' | relative_path}}"
      alt="Element wise
    representation of (mxn) matrix"
      style="max-width: -webkit-fill-available"
      width="50%"
      height="50%"
    />
  </div>
  <p>
    For example, an O(mn) algorithm is one that iterates through every element
    in a matrix. The function below prints out all the elements in a matrix to
    the console, where the matrix can have dimensions m x n.
  </p>
  <pre>
      {% highlight python %}
def print_matrix(matrix):
    m,n = matrix.shape
    for i in range(m):
        for j in range(n):
            print(matrix[i][j])
      {% endhighlight %}
    </pre
  >
  <p>
    It's important to note a few facts about O(mn). First, while O(mn) is
    commonly used to express the time complexity of an algorithm with two
    variables, other notations such as O(m + n), O(m log n), or O(n log m) may
    be more appropriate in certain cases. We must carefully choose the correct
    way to sum or multiply the variables based on the steps our algorithm
    performs.
  </p>

  <p>
    Secondly, it's important to note that O(mn) can result in a poor or average
    runtime, particularly when m and n are large. In such cases, the runtime
    behaves more like O(n<sup>2</sup>). On the other hand, if one variable is
    significantly large, and the other is very small, the runtime behaves more
    like O(n) or O(m), depending on which variable is larger. The sizes of m and
    n can have a significant impact on the execution time. As demonstrated in
    the plot below, the execution speed of traversing an m x n matrix slows down
    as the size of m and n increase.
  </p>
  <iframe
    width="100%"
    height="600"
    frameborder="0"
    seamless="seamless"
    scrolling="no"
    src="/charts_and_graphs/Omn_example.html"
  ></iframe>
  <p>
    Overall, the O(mn) notation is helpful to understand the time complexity of
    an algorithm that is dependent on two variables. It's important to consider
    the notation when dealing matrices or two-dimensional arrays, especially
    when their size is large.
  </p>

  <hr />
  <h4>O(log n)</h4>
  <p>
    O(log n) is a measure of logarithmic time complexity, indicating that the
    time taken by an algorithm grows in proportion to the logarithm of the input
    size. As the input size increases, the time taken by the algorithm will
    increase much slower than linearly, making it efficient for large inputs.
    For instance, if an algorithm takes 10 seconds to process an input of size
    1,000, it would take only around 20 seconds to process an input of size
    1,000,000, which is a significantly smaller increase in time compared to the
    linear case.
  </p>

  <p>
    A popular example of an algorithm with O(log n) complexity is binary search.
    It repeatedly divides a sorted array in half until the target value is
    found. As the size of the array doubles, the number of steps needed to find
    the target value increases only by one, making binary search an extremely
    efficient algorithm for large inputs. An implementation and animation are
    provided below, but don't worry too much about understanding the code just
    yet. (This algorithm will be discussed in detail in a future post).
  </p>
  <div style="text-align: center">
    <a
      title="Mazen Embaby, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons"
      href="https://commons.wikimedia.org/wiki/File:Binary-search-work.gif"
      ><img
        width="60%"
        alt="Binary-search-work"
        src="https://upload.wikimedia.org/wikipedia/commons/c/c1/Binary-search-work.gif"
    /></a>
  </div>
  <pre>
      {% highlight python %}
def binary_search(array, value):

    low = 0
    high = len(array) - 1

    while low <= high:
        middle = (low + high) // 2

        if array[middle] == value:
            return middle

        # if midpoint is smaller, ignore the left half
        if array[middle] < value:
            low = middle + 1

        # if midpoint is larger, ignore the right half
        if array[middle] > value:
            high = middle - 1

    # return -1 if match not found
    return -1
      {% endhighlight %}
  </pre>
  <p>
    The graph below shows the performance of our O(log n)
    <code>binary_search()</code> algorithm compared to O(n)
    <code>linear search()</code>
    as the size of our input array increases increases. As expected, the O(log
    n) algorithm has a much slower increase in time as the input size increases.
  </p>
  <iframe
    width="100%"
    height="500"
    frameborder="0"
    seamless="seamless"
    scrolling="no"
    src="/charts_and_graphs/binary_vs_linear_search.html"
  ></iframe>

  <p>
    For those who are interested in a mathematical explanation of the
    logarithmic function, recall that log<sub>b</sub>(n) tells us how many times
    we can subdivide n by b. For example, if we wanted to know how many times we
    can divide 100 by 2, we simply calculate log<sub>2</sub>(100), which equals
    6.44 (approximately). This means that binary search would subdivide an array
    of size 100 at most 7 times to find a maximum value, making it a very
    efficient algorithm for large inputs.
  </p>
  <hr />
  <h4>O(n log n)</h4>
  <p>
    Understanding the concept of O(log n) helps in comprehending the time
    complexity of algorithms with O(n log n). Such algorithms execute O(log n)
    operations n times. Sorting algorithms, like merge sort and quicksort, are
    common examples of algorithms with an average-case time complexity of O(n
    log n). Although O(n log n) is more complex than O(n), it is still
    considered a fairly good runtime for large input sizes because the
    logarithmic factor grows much slower than linearly.
  </p>
  <p>
    We'll explore specific O(n log n) algorithms in future posts, but for now,
    it's crucial to understand that the time complexity of O(n log n) scales
    proportionally to n times the logarithm of n.
  </p>
  <hr />
  <h4>O(2<sup>n</sup>)</h4>
  <p>
    The notation O(2<sup>n</sup>) represents an algorithm's time complexity that
    grows exponentially as the input size increases. This notation indicates
    that the algorithm's runtime doubles with each additional input element.
    This exponential growth can lead to significant performance issues when
    dealing with large input sizes.
  </p>
  <p>
    An example of an algorithm with O(2<sup>n</sup>) time complexity is the
    recursive Fibonacci function, which calculates the n<sup>th</sup>
    <a href="https://en.wikipedia.org/wiki/Fibonacci_number"
      >Fibonacci number</a
    >
    by recursively calling itself for the two preceding numbers in the sequence.
    Here's an implementation in Python:
  </p>
  <pre>
  {% highlight python %}
def fibonacci(n):
    if n == 0 or n == 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
  {% endhighlight %}
</pre
  >
  <p>
    The time complexity of this implementation is O(2<sup>n</sup>), since each
    recursive call generates two additional recursive calls until it reaches the
    base case. Therefore, the number of recursive calls doubles with each
    increment of n.
  </p>
  <div style="text-align: center">
    <img
      src="{{ '/images/fibonacci.png' | relative_path}}"
      alt="Recursive tree of
    fibonacci sequence"
      style="max-width: -webkit-fill-available"
      width="100%"
      height="100%"
    />
  </div>

  <p>
    This rapid growth in recursive calls leads to an exponential growth in time
    as demonstrated in the plot below.
  </p>

  <iframe
    width="100%"
    height="600"
    frameborder="0"
    seamless="seamless"
    scrolling="no"
    src="/charts_and_graphs/O2n_example.html"
  ></iframe>
  <p>
    It's worth noting that O(2<sup>n</sup>) is considered an inefficient time
    complexity for most real-world applications. Algorithms with this time
    complexity typically have an exponential growth rate that quickly becomes
    unmanageable for larger input sizes.
  </p>
  <p>
    In conclusion, understanding the time complexity of O(2<sup>n</sup>) is
    important for identifying algorithms with exponential growth rates. Avoiding
    these types of algorithms can help improve the performance of your code and
    prevent performance issues when dealing with large input sizes.
  </p>
  <hr />
  <h4>O(n!)</h4>
  <p>
    O(n!) represents factorial time complexity, where the runtime of the
    algorithm grows as a factorial of the input size. This is one of the most
    inefficient time complexities, as it grows very quickly and is often
    impractical for larger inputs.
  </p>
  <p>
    An algorithm with O(n!) time complexity will execute n times for each
    element in the input, leading to a total of n! (n factorial) iterations. For
    example, an algorithm that generates all permutations of an input sequence
    would have a time complexity of O(n!), as the number of permutations of n
    elements is n!.
  </p>
  <p>
    Due to the exponential growth of O(n!), it's often only practical for small
    inputs. For larger inputs, the algorithm can take an extremely long time to
    run, making it unfeasible for most real-world applications. In general, it's
    best to avoid using algorithms with O(n!) time complexity unless absolutely
    necessary, and to explore more efficient alternatives whenever possible.
  </p>
  <hr />
  <h4>Good, Bad, and Ugly</h4>
  <p>
    By now, you should have a solid understanding of how to determine the time
    complexity of algorithms. However, you may still wonder what constitutes an
    exceptional time complexity. As a general rule of thumb, it's best to avoid
    any algorithm with a time complexity worse than O(n<sup>2</sup>). Although
    sometimes we may not be able to avoid using inefficient algorithms, it's
    crucial to be mindful of the size of inputs we provide. Our goal is to
    design algorithms with the best possible time complexity while still
    achieving the desired results.
  </p>
  <p>
    While it's ideal to avoid algorithms with a time complexity worse than
    O(n<sup>2</sup>), there are cases where it's unavoidable. In these
    situations, it's essential to be mindful of the size of your inputs and
    optimize your code for the best possible performance while still delivering
    the expected results. There are techniques that will be introduced in later
    post that can help optimize code and reduce its time complexity.
  </p>

  <hr />
  <h4>Conclusion</h4>
  <p>
    In conclusion, Big-O notation is an essential tool for analyzing the
    performance of algorithms. By using Big-O notation, we can estimate how an
    algorithm will behave as its input size grows, allowing us to identify
    bottlenecks and inefficiencies in our code. By understanding the various
    Big-O classifications and their implications, we can make informed decisions
    about which algorithms to use in different situations. While the
    mathematical theory behind Big-O notation can be complex, it is essential to
    have a basic understanding of it to be an effective software engineer. I
    hope this post has provided you with a clear understanding of Big-O notation
    and its significance in computer science, allowing you to write more
    efficient and optimized code. Remember, always analyze the time complexity
    of your algorithms, and use the appropriate algorithm for the task at hand.
  </p>
</div>
