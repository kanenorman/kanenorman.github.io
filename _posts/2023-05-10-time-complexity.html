---
layout: posts
title: "Big-O Notation"
author: Kane Norman
categories: [programming]
excerpt: How to measure efficiency and scalability
order: 2
toc: true
---

<div class="discussion-content">
  <h4>Introduction</h4>

  <p>
    In the
    <a
      href="{{ '/programming/data-structures-and-algorithms' | relative_url }}"
      target="_blank"
      >previous post</a
    >
    , I mentioned that it is difficult to predict the
    <span style="font-style: italic; font-weight: bold; color: darkorange"
      >exact</span
    >
    amount of time an algorithm takes to execute. Since it is difficult to
    predict the exact execution time, we often focus on understanding how the
    execution time scales as the size of the input(s) increases. For instance,
    will an algorithm that sorts 10 elements take longer to sort 1,000,000
    elements? If so, can we quantify how much longer it will take? This is where
    Big-O notation comes into play. Big-O notation provides a way to describe an
    algorithm's expected execution time in terms of the number or magnitude of
    it inputs.
  </p>
  <p>
    The graph presented below illustrates several common Big-O notations,
    showcasing the relationship between the number of operations (y-axis) and
    the size or magnitude of the inputs (x-axis). By analyzing the graph, we can
    observe distinct complexities such as O(log n) which require a relatively
    small number of operations even as the number of inputs grows. Conversely,
    complexities like O(n!) exhibit a rapid increase in the number of required
    operations as the input size increases. This rapid growth can result in poor
    performance when handling large amounts of data. Hence, it is crucial to
    recognize situations where the Big-O notation warns us about the potential
    for a significant number of operations and the subsequent impact on
    performance.
  </p>

  <div style="text-align: center">
    <figure>
      <img
        src="{{ '/images/Big-O-graph.png' | relative_path}}"
        alt="Comparison of
      Big-0 time complexities"
        style="max-width: -webkit-fill-available"
      />
      <figcaption>
        Image courtesy of
        <a href="https://www.bigocheatsheet.com" target="_blank"
          >bigocheatsheet.com</a
        >
      </figcaption>
    </figure>
  </div>

  <p>
    In this post, we will explore the importance of Big-O notation in analyzing
    algorithm performance. Although the mathematical theory behind Big-O
    notation can be complicated, I will provide simple, easy-to-understand
    examples to help you grasp the basics of Big-O notation. By using real-world
    scenarios, I will demonstrate how Big-O notation can help you compare the
    efficiency of different algorithms and make informed decisions when
    implementing them. By the end of this post, you should have a solid
    understanding of Big-O notation and its significance in computer science.
  </p>
  <p>
    In this article, our primary focus will be on time complexity, but it's
    important to note that a similar analysis can be applied to space complexity
    as well. In future posts, we will dive into the topic of space complexity in
    more detail.
  </p>
  <hr />
  <h4>Mathematical Preliminaries</h4>
  <p>
    Before diving into the details of Big-O notation, it is essential to grasp
    the fundamentals of a few basic mathematical functions that frequently arise
    when determining time complexity:
  </p>

  <ul>
    <li>
      <span style="color: purple; font-weight: bold">Constant Functions </span>:
      These are the simplest type of functions, defined as
      <span class="mathematical">f(x) = c</span>, where
      <span class="mathematical">c</span> is a constant value. In other words,
      regardless of the input <span class="mathematical">x</span>, the output
      remains the same. Graphically, constant functions are represented by a
      horizontal line, indicating that their value does not change with
      different inputs. These functions are particularly significant in
      algorithm analysis as they represent operations that execute in a fixed
      amount of time, regardless of the input size.
    </li>
    <li>
      <span style="color: purple; font-weight: bold">Linear Functions </span>:
      These are first-degree polynomials in the form of
      <span class="mathematical">f(x) = m*x + b</span>, where
      <span class="mathematical">m</span> and
      <span class="mathematical">b</span> are constant values indicating the
      slope and y-intercept, respectively. The graph of a linear function is a
      straight line, with the steepness of the line determined by the value of
      <span class="mathematical">m</span>. When
      <span class="mathematical">m</span> is large, the line is steep, and when
      <span class="mathematical">m</span> is small (near zero), the line is
      relatively flat.
    </li>
    <li>
      <span style="color: purple; font-weight: bold">Quadratic Functions </span
      >: These functions are second-degree polynomials that often represent
      curved relationships between variables. They have the general form
      <span class="mathematical">f(x) = a*x<sup>2</sup> + b*x + c</span>, where
      <span class="mathematical">a</span>, <span class="mathematical">b</span>,
      and <span class="mathematical">c</span> are constants determining the
      shape and position of the graph. Graphically, quadratic functions
      typically exhibit a curved shape, with slow growth for small values of
      <span class="mathematical">x</span> near zero and rapid growth as
      <span class="mathematical">x</span> increases.
    </li>
    <li>
      <span style="color: purple; font-weight: bold"
        >Logarithmic Functions </span
      >: These functions are represented as
      <span class="mathematical">f(x) = log<sub>b</sub>(x)</span>. It roughly
      answers the question: "How many times can we divide
      <span class="mathematical">x</span> by
      <span class="mathematical">b</span>?".Graphically, logarithmic functions
      exhibit rapid growth initially and then slowdown as
      <span class="mathematical">x</span> increases.
    </li>
    <li>
      <span style="color: purple; font-weight: bold"
        >Exponential Functions </span
      >: These functions are defined as
      <span class="mathematical">f(x) = a<sup>x</sup></span
      >, where <span class="mathematical">a</span> is a constant base. These
      functions exhibit rapid growth as
      <span class="mathematical">x</span> increases. As a side note, this
      function has an inverse relationship to the logarithmic function.
    </li>
    <li>
      <span style="color: purple; font-weight: bold">Factorial Functions </span
      >: These are denoted as <span class="mathematical">f(x) = x!</span> and
      represent the product of all positive integers from 1 to
      <span class="mathematical">x</span>. Factorial functions arise in
      combinatorial mathematics, especially when counting permutations and
      combinations. These functions grow rapidly as
      <span class="mathematical">x</span> increases, with the graph exhibiting
      steep growth.
    </li>
  </ul>
  <div style="text-align: center">
    <figure>
      <img
        src="{{ '/images/math-functions.png' | relative_path}}"
        alt="Comparison of Preliminary Functions"
        style="max-width: -webkit-fill-available"
        height="90%"
        width="90%"
      />
      <figcaption>Graphs of key elementary mathematical functions</figcaption>
    </figure>
  </div>

  <p>
    While you don't need a deep understanding of these functions, it is
    important to have a basic understanding of their behavior as
    <span class="mathematical">x</span> approaches infinity. This knowledge
    becomes particularly valuable when comparing the growth rate of algorithms
    to these functions. Understanding the growth patterns and relative rates of
    these functions enables you to make insightful comparisons and assess the
    expected runtime of different algorithms. As a general rule of thumb,
    algorithms with constant runtime are the fastest, followed by logarithmic,
    linear, quadratic, exponential, then factorial. The graph shown in the
    introduction illustrates these differences clearly, so you can refer to it
    as you familiarize yourself with these functions. You will see later, being
    able to relate algorithmic performance to these familiar growth patterns
    will assist you in calculating Big-O notation.
  </p>

  <hr />
  <h4>Best, Average, and Worst Case</h4>
  <p>
    When analyzing the performance of algorithms, we may encounter input
    arguments that meet special conditions and significantly affect performance.
    Some inputs can create optimal performance conditions, resulting in far
    better runtime than the average case, while others can create worst-case
    conditions, leading to significantly worse performance. Therefore, it is
    important to consider the range of inputs that our algorithm may encounter
    when evaluating its performance. To ensure a fair evaluation, we can
    calculate three different time complexities for an algorithm:
  </p>
  <ul>
    <li>
      <span style="color: purple; font-weight: bold"
        >Best-case time complexity </span
      >: represents the minimum possible runtime of an algorithm under ideal
      conditions. It occurs when the algorithm's performance is optimal due to
      input values that make it run faster than usual.
    </li>
    <li>
      <span style="color: purple; font-weight: bold"
        >Average-case time complexity </span
      >: represents the performance we can expect on average when the input does
      not meet any exceptional conditions.
    </li>
    <li>
      <span style="color: purple; font-weight: bold"
        >Worst-case time complexity </span
      >: represents the maximum possible runtime of an algorithm under the least
      favorable conditions. It occurs when the algorithm's performance is the
      worst due to input values that make it run slower than usual.
    </li>
  </ul>

  <p>
    Here is an example to demonstrate these three cases. Consider the Python
    List method
    <code>index()</code>, which searches for a target value in a Python List and
    returns the index of its first occurrence. The performance of the method can
    be evaluated based on the best, average, and worst-case scenarios:
  </p>

  <ul>
    <li>
      Best-case scenario: occurs when the element we are looking for is the
      first element in the list. We won't have to search through any other
      values in the list, and the method can return almost immediately,
      resulting in the minimum possible runtime.
    </li>
    <li>
      Average-case scenario: occurs when the value we are searching for is
      located somewhere in the middle of the list. In this scenario, we will
      have to search through about half the elements in the list until we find
      our target value, resulting in an average runtime.
    </li>
    <li>
      Worst-case scenario: occurs when the element is not in the list or occurs
      towards the end. In this case, the method will have to search through a
      large portion of the list, leading to the longest possible runtime.
    </li>
  </ul>

  <pre>
{%highlight python%}
# list that looks like [0, 1, 2, 3, ...97, 98, 99]
numbers = [number for number in range(100)]

# specific elements in array to use as inputs
first = 0
middle = 50
last = 99

# create three scenarios based on inputs
best_case = numbers.index(first)
average_case = numbers.index(middle)
worst_case = numbers.index(last)
{%endhighlight%}
  </pre>
  <p>
    Below we can see the execution times for the <code>best_case</code>,
    <code>average_case</code>, and <code>worst_case</code> scenarios from the
    code snippet above. As expected, the best-case scenario executed the
    fastest, followed by the average case and the worst-case, respectively.
  </p>

  <div style="text-align: center">
    <figure>
      <img
        src="{{ '/images/best-average-worst-case.png' | relative_path}}"
        alt="Execution times of best, average, and worst case"
        style="max-width: -webkit-fill-available"
        height="70%"
        width="70%"
      />
    </figure>
  </div>

  <p>
    By understanding the performance characteristics of the
    <code>index()</code> method under different scenarios, we can make informed
    decisions about how to use it in our code. Overall, understanding the best,
    average, and worst-case scenarios can help us to optimize our code and avoid
    unexpected performance issues.
  </p>

  <p>
    In this post, we'll primarily focus on the worst-case time complexity,
    big-0, as it provides an upper bound on the amount of time that an algorithm
    will take to complete. Understanding the worst-case scenario is particularly
    important in time-critical applications, where we need to guarantee that the
    algorithm will complete within a certain time frame. However, we should keep
    in mind that the worst-case scenario may not always be the most common or
    representative of real-world performance. Therefore, we should also consider
    the average-case scenario to ensure that our algorithm meets performance
    requirements in typical use cases.
  </p>
  <hr />
  <h4>Rules for Big-O Notation</h4>
  We calculate big-O using the following set of rules:
  <ul>
    <li>
      <span style="color: purple; font-weight: bold"
        >Ignore non-dominant terms and constants </span
      >: When calculating the time complexity of an algorithm, we can safely
      ignore the non-dominant terms and constants. This allows us to focus on
      the most significant factors that affects the run time of the algorithm.
      For example, O(2n + 1) can be simplified to O(n), and O(n<sup>2</sup> + n)
      can be simplified to O(n<sup>2</sup>) By doing so, we only focus on the
      most dominant term(s) that significantly impact runtime performance and
      provide a general expected runtime rather than a specific one.
    </li>
    <li>
      <span style="color: purple; font-weight: bold"
        >Treat different inputs as separate variables </span
      >: When analyzing the time complexity of an algorithm, it's important to
      consider the impact of multiple inputs. Some algorithms may take
      two-dimensional inputs, or multiple function arguments. It's important to
      treat these inputs as separate variables when determining the Big-O
      notation.
    </li>
    <li>
      <span style="color: purple; font-weight: bold"
        >Sum or multiply steps </span
      >: When an algorithm has multiple steps, we can either sum or multiply the
      complexities of each step to obtain the overall time complexity. For
      example, if an algorithm repeats an O(log n) operation n times, the
      overall time complexity would be O(n) x O(log n) = O(n log n).
    </li>
  </ul>
  <p>
    It's important to keep in mind that the purpose of Big-O notation is to
    provide a general understanding of how an algorithm scales. We are not
    concerned with the exact time it takes to execute an algorithm, but rather
    how it behaves as the input size grows
  </p>

  <hr />
  <h4>O(1)</h4>
  <p>
    O(1) represents constant time complexity. This means that the time required
    by the algorithm does not change with the size of the input. Instead, the
    code executes at a constant rate. For instance, consider the following code
    snippet:
  </p>
  <pre>
  {%highlight python%}
from typing import Any, List

def get_first_element(array: List[Any]) -> Any:
    """
    Get the first element from the given array.

    Parameters
    ----------
    array : List[Any]
        A list of elements.

    Returns
    -------
    Any
        The first element of the array.
    """
    return array[0]

  {%endhighlight%}
</pre
  >

  <p>
    The function <code>get_first_element()</code> is a simple operation that
    retrieves the first element of an input array. It achieves this by directly
    accessing the base memory address of the array and returning the value
    stored there. Since the number of operations performed remains constant
    regardless of the array size, the function runs in constant time. This means
    that regardless of whether the array contains one element or one million
    elements, the execution time of <code>get_first_element()</code> remains the
    same. On my computer, this function typically executes in approximately 4e-5
    seconds, regardless of the array size. It's important to note that the exact
    execution time may vary slightly due to background processes, but it
    consistently falls within this range.
  </p>

  <p>
    As another important note, earlier that we said we drop constants when
    reporting Big-O. O(1) is the special case that breaks this rule. Any time we
    have constant runtime we report it as O(1).
  </p>

  <hr />
  <h4>O(n)</h4>
  <p>
    O(n) represents linear time complexity, which means that the time taken by
    an algorithm increases in direct proportion to the size of the input. In
    other words, if the input size increases by a factor of k, then the time
    taken by the algorithm will also increase by a factor of k. For example, if
    an algorithm takes 5 seconds to process an array with 10 elements, it would
    take around 50 seconds to process an array with 100 elements and around 500
    seconds to process an array with 1,000 elements.
  </p>
  <p>
    A simple example to illustrate O(n) time complexity is the following
    function:
  </p>
  <pre>
    {% highlight python %}
from typing import List

def sum_array(array: List[int]) -> int:
    """
    Compute the sum of all numbers in the given array.

    Parameters
    ----------
    array : List[int]
        A list of integers.

    Returns
    -------
    int
        The sum of all numbers in the array.
    """
    total = 0
    for number in array:
        total += number
    return total

    {% endhighlight %}
</pre
  >
  <p>
    In this example, the number of operations performed is directly proportional
    to the size of the input array. If the array contains n elements, n
    additions will need to be performed. The graph below shows the performance
    of the function <code>sum_array()</code> as the size of the input increases.
    As expected, the relationship between input size and average execution time
    is clearly linear.
  </p>

  <div style="text-align: center">
    <figure>
      <img
        src="{{ '/images/linear-complexity.png' | relative_path}}"
        alt="Execution time of sum_array()"
        style="max-width: -webkit-fill-available"
        height="70%"
        width="70%"
      />
    </figure>
  </div>

  <p>
    Another example of a O(n) algorithm is the linear search algorithm, which
    iterates through an array until it finds the value it is looking for
    (similar to the Python <code>index()</code> method we discussed earlier).
    Here's an implementation of the algorithm in Python:
  </p>
  <pre>
    {% highlight python %}
from typing import List, Any

def linear_search(array: List[Any], value: Any) -> int:
    """
    Perform a linear search to find the index of the given value in the array.

    Parameters
    ----------
    array : List[Any]
        A list of elements.
    value : Any
        The value to search for in the array.

    Returns
    -------
    int
        The index of the value in the array, or -1 if the value is not found.
    """
    for i in range(len(array)):
        if array[i] == value:
            return i
    return -1

    {% endhighlight %}
</pre
  >
  <p>
    In the worst-case scenario for a linear search algorithm, the target value
    is located at the end of the array. This means the algorithm would need to
    perform n iterations to find our target value, where n is the number of
    elements in the array. Therefore, the worst-case time complexity of the
    linear search algorithm is O(n).
  </p>
  <p>
    Here's a tricky question: If we add an extra constant operation to the
    function <code>sum_array()</code> like the code snippet below, would the
    time complexity still be considered O(n), or would it be considered O(n+1)?
  </p>
  <pre>
    {% highlight python %}
from typing import List

def sum_array(array: List[int]) -> int:
    """
    Calculate the sum of all numbers in the given array.

    Parameters
    ----------
    array : List[int]
        A list of integers.

    Returns
    -------
    int
        The sum of all numbers in the array.
    """
    total = 0
    for number in array:
        total += number

    # Adding a constant operation
    total = total + 1

    return total

    {% endhighlight %}
</pre
  >
  <p>
    It would still be considered O(n) time complexity. The reason why is because
    we drop constants when calculating Big-O notation. Remember, the purpose of
    Big-O is to measure how the algorithm scales as the input size increases,
    and we are only concerned with the general trend of the algorithm's
    behavior. In other words, we don't care about the exact amount of time the
    algorithm takes to execute, but rather about how that time changes as the
    input size grows.
  </p>
  <p>
    To put it in mathematical terms, we don't care if the time a function takes
    to execute is modeled by the function
    <span class="mathematical">f(x) = x, f(x) = ax, f(x) = x + b,</span> or
    <span class="mathematical">f(x) = ax + b</span>. All we care about is that
    the function is a linear function that can be described proportionally to
    <span class="mathematical">x</span>.
  </p>

  <hr />
  <h4>O(n<sup>2</sup>)</h4>
  <p>
    O(n<sup>2</sup>) represents quadratic time complexity, meaning that the
    algorithm scales proportional to the square of the size of the input. In
    other words, if the input size increases by a factor of k, then the time
    taken by the algorithm will increase by a factor of k<sup>2</sup>. For
    example, if an algorithm takes 100 second to process an array with 10
    elements, it would take around 10,000 seconds to process an array with 100
    elements and around 1,000,000 seconds to process an array with 1,000
    elements.
  </p>
  <p>
    A common example of an algorithm with quadratic time complexity is the
    nested loop, where a loop runs inside another loop. An example of such an
    algorithm is the following function that finds all the pairs of numbers in
    an array that add up to a target sum:
  </p>
  <pre>
    {% highlight python %}
from typing import List, Tuple

def find_pairs_with_sum(array: List[int], target_sum: int) -> List[Tuple[int, int]]:
    """
    Find pairs of numbers in the given array that sum up to the target sum.

    Parameters
    ----------
    array : List[int]
        A list of integers.
    target_sum : int
        The target sum.

    Returns
    -------
    List[Tuple[int, int]]
        A list of tuples representing pairs of numbers that sum up to the target sum.
    """
    n = len(array)
    pairs = []

    for i in range(n):
        for j in range(i + 1, n):
            if array[i] + array[j] == target_sum:
                pairs.append((array[i], array[j]))

    return pairs

    {% endhighlight %}
  </pre>
  <p>
    In this function, for each element in the array, it checks every other
    element to see if their sum is equal to the target sum. We can calculate the
    number of comparisons this algorithm preforms in the table below:
  </p>

  <table class="tg">
    <thead>
      <tr>
        <th>i</th>
        <th>j</th>
        <th>comparisons</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0</td>
        <td>[1,n)</td>
        <td>n-1</td>
      </tr>
      <tr>
        <td>1</td>
        <td>[2,n)</td>
        <td>n-2</td>
      </tr>
      <tr>
        <td>2</td>
        <td>[3,n)</td>
        <td>n-3</td>
      </tr>
      <tr>
        <td>...</td>
        <td>...</td>
        <td>...</td>
      </tr>
      <tr>
        <td>n-2</td>
        <td>[n-1,n)</td>
        <td>1</td>
      </tr>
      <tr>
        <td>n-1</td>
        <td>[n,n)</td>
        <td>0</td>
      </tr>
    </tbody>
  </table>
  <p>
    Thus, the number of comparisons made by this algorithm can be expressed as
    the sum of the series (n-1) + (n-2) + (n-3) + ... + 1 + 0, which is
    equivalent to (n<sup>2</sup>-n)/2. Therefore, using the Big-O notation
    rules, we can say that the number of operations performed by this algorithm
    is proportional to the square of the input size, making its time complexity
    O(n<sup>2</sup>). As the input size grows, the time taken by the algorithm
    increases quadratically. The graph below shows the performance of the
    function
    <code>find_pairs_with_sum()</code> as the size of the input array increases.
    As expected, the relationship between input size and time is clearly
    quadratic.
  </p>

  <div style="text-align: center">
    <figure>
      <img
        src="{{ '/images/quadratic-complexity.png' | relative_path}}"
        alt="Execution time of find_pairs_with_sum()"
        style="max-width: -webkit-fill-available"
        height="70%"
        width="70%"
      />
    </figure>
  </div>
  <hr />
  <h4>O(mn)</h4>

  <p>
    The notation O(mn) indicates that the time complexity of an algorithm is
    proportional to both m and n. This notation is commonly used to express the
    time complexity of an algorithm with two variables, particularly when
    working with matrices or two-dimensional arrays.
  </p>
  <div style="text-align: center">
    <img
      src="{{ '/images/mxn_matrix.png' | relative_path}}"
      alt="Element wise
    representation of (mxn) matrix"
      style="max-width: -webkit-fill-available"
      width="50%"
      height="50%"
    />
  </div>
  <p>
    For example, an O(mn) algorithm is one that iterates through every element
    in a matrix. The function below prints out all the elements in a matrix to
    the console, where the matrix can have dimensions m x n.
  </p>
  <pre>
      {% highlight python %}
import numpy as np

def print_matrix(matrix: np.ndarray) -> None:
    """
    Print the elements of a matrix.

    Parameters
    ----------
    matrix : np.ndarray
        A NumPy array representing the matrix.

    Returns
    -------
    None
        This function does not return anything.
    """
    m, n = matrix.shape
    for row in range(m):
        for column in range(n):
            print(matrix[row][column])

      {% endhighlight %}
    </pre
  >
  <p>
    It's important to note a few facts about O(mn). First, while O(mn) is
    commonly used to express the time complexity of an algorithm with two
    variables, other notations such as O(m + n), O(m log n), or O(n log m) may
    be more appropriate in certain cases. We must carefully choose the correct
    way to sum or multiply the variables based on the steps our algorithm
    performs.
  </p>

  <p>
    Secondly, it's important to note that O(mn) can result in a poor or average
    runtime, particularly when m and n are large. In such cases, the runtime
    behaves more like O(n<sup>2</sup>). On the other hand, if one variable is
    significantly large, and the other is very small, the runtime behaves more
    like O(n) or O(m), depending on which variable is larger. The sizes of m and
    n can have a significant impact on the execution time. As demonstrated in
    the plot below, the execution speed of traversing an m x n matrix slows down
    as the size of m and n increase.
  </p>
  <div style="text-align: center">
    <img
      src="{{ '/images/matrix-traverse.png' | relative_path}}"
      alt="Traversal speed of (mxn) matrix"
      style="max-width: -webkit-fill-available"
      width="80%"
      height="80%"
    />
  </div>
  <p>
    Overall, the O(mn) notation is helpful to understand the time complexity of
    an algorithm that is dependent on two variables. It's important to consider
    the notation when dealing matrices or two-dimensional arrays, especially
    when their size is large.
  </p>

  <hr />
  <h4>O(log n)</h4>
  <p>
    O(log n) is a measure of logarithmic time complexity, indicating that the
    time taken by an algorithm grows in proportion to the logarithm of the input
    size. As the input size increases, the time taken by the algorithm will
    increase much slower than linearly, making it efficient for large inputs.
    For instance, if an algorithm takes 10 seconds to process an input of size
    1,000, it would take only around 20 seconds to process an input of size
    1,000,000, which is a significantly smaller increase in time compared to the
    linear case.
  </p>

  <p>
    A popular example of an algorithm with O(log n) complexity is binary search.
    It repeatedly divides a sorted array in half until the target value is
    found. As the size of the array doubles, the number of steps needed to find
    the target value increases only by one, making binary search an extremely
    efficient algorithm for large inputs. An implementation and animation are
    provided below, but don't worry too much about understanding the code just
    yet. (This algorithm will be discussed in detail in a future post).
  </p>
  <div style="text-align: center">
    <a
      title="Mazen Embaby, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons"
      href="https://commons.wikimedia.org/wiki/File:Binary-search-work.gif"
      target="_blank"
      ><img
        width="50%"
        alt="Binary-search-work"
        src="https://upload.wikimedia.org/wikipedia/commons/c/c1/Binary-search-work.gif"
    /></a>
  </div>
  <pre>
      {% highlight python %}
from typing import List

def binary_search(array: List, target) -> int:
    """
    Perform binary search on a sorted array to find the index of a target value.
    
    Parameters:
    ----------
    array: List
        A sorted list of integers or floats.
    target:
        The value to search for in the array.
    
    Returns:
    --------
    int
        The index of the target value in the array if found, otherwise -1.

    Example:
    --------
    >>> array = [2, 5, 7, 10, 14, 19, 20]
    >>> target = 10
    >>> binary_search(array, target)
    3

    """
    low = 0
    high = len(array) - 1

    while low <= high:
        middle = (low + high) // 2

        if target == array[middle]:
            return middle

        # If target is larger than midpoint, ignore the left half
        if target > array[middle]:
            low = middle + 1

        # If target is smaller than midpoint, ignore the right half
        if target < array[middle]:
            high = middle - 1

    # Return -1 if match not found
    return -1
      {% endhighlight %}
  </pre>
  <p>
    The graph below shows the performance of our O(log n)
    <code>binary_search()</code> algorithm compared to O(n)
    <code>linear search()</code>
    as the size of our input array increases increases. As expected, the O(log
    n) algorithm has a much slower increase in time as the input size increases.
  </p>
  <div style="text-align: center">
    <img
      src="{{ '/images/binary_vs_linear_search.png' | relative_path}}"
      alt="Binary vs linear search"
      style="max-width: -webkit-fill-available"
      width="80%"
      height="80%"
    />
  </div>

  <p>
    For those who are interested in a mathematical explanation of the
    logarithmic function, recall that
    <span class="mathematical">log<sub>b</sub>(n)</span> tells us how many times
    we can subdivide <span class="mathematical">n</span> by
    <span class="mathematical">b</span>. For example, if we wanted to know how
    many times we can divide 100 by 2, we simply calculate log<sub>2</sub>(100),
    which equals 6.44 (approximately). This means that binary search would
    subdivide an array of size 100 at most 7 times to find a maximum value,
    making it a very efficient algorithm for large inputs.
  </p>
  <hr />
  <h4>O(n log n)</h4>
  <p>
    Understanding the concept of O(log n) helps in comprehending the time
    complexity of algorithms with O(n log n). Such algorithms execute O(log n)
    operations n times. Sorting algorithms, like merge sort and quicksort, are
    common examples of algorithms with an average-case time complexity of O(n
    log n). Although O(n log n) is more complex than O(n), it is still
    considered a fairly good runtime for large input sizes because the
    logarithmic factor grows much slower than linearly.
  </p>
  <p>
    We'll explore specific O(n log n) algorithms in future posts, but for now,
    it's crucial to understand that the time complexity of O(n log n) scales
    proportionally to n times the logarithm of n.
  </p>
  <hr />
  <h4>O(2<sup>n</sup>)</h4>
  <p>
    The notation O(2<sup>n</sup>) represents an algorithm's time complexity that
    grows exponentially as the input size increases. This notation indicates
    that the algorithm's runtime doubles with each additional input element.
    This exponential growth can lead to significant performance issues when
    dealing with large input sizes.
  </p>
  <p>
    An example of an algorithm with O(2<sup>n</sup>) time complexity is the
    recursive Fibonacci function, which calculates the n<sup>th</sup>
    <a href="https://en.wikipedia.org/wiki/Fibonacci_number" target="_blank"
      >Fibonacci number</a
    >
    by recursively calling itself for the two preceding numbers in the sequence.
    Here's an implementation in Python:
  </p>
  <pre>
  {% highlight python %}
def fibonacci(n: int) -> int:
    """
    Calculate the Fibonacci number for the given input.

    Parameters
    ----------
    n : int
        The position of the Fibonacci number to calculate.

    Returns
    -------
    int
        The Fibonacci number at the specified position.

    """
    if n == 0 or n == 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
  {% endhighlight %}
</pre
  >
  <p>
    The time complexity of this implementation is O(2<sup>n</sup>), since each
    recursive call generates two additional recursive calls until it reaches the
    base case. Therefore, the number of recursive calls doubles with each
    increment of n.
  </p>
  <div style="text-align: center">
    <img
      src="{{ '/images/fibonacci.png' | relative_path}}"
      alt="Recursive tree of
    fibonacci sequence"
      style="max-width: -webkit-fill-available"
      width="100%"
      height="100%"
    />
  </div>

  <p>
    This rapid growth in recursive calls leads to an exponential growth in time
    as demonstrated in the plot below.
  </p>

  <div style="text-align: center">
    <img
      src="{{ '/images/exponential_complexity.png' | relative_path}}"
      alt="Performance of fibonacci function"
      style="max-width: -webkit-fill-available"
      width="80%"
      height="80%"
    />
  </div>
  <p>
    It's worth noting that O(2<sup>n</sup>) is considered an inefficient time
    complexity for most real-world applications. Algorithms with this time
    complexity typically have an exponential growth rate that quickly becomes
    unmanageable for larger input sizes.
  </p>
  <p>
    In conclusion, understanding the time complexity of O(2<sup>n</sup>) is
    important for identifying algorithms with exponential growth rates. Avoiding
    these types of algorithms can help improve the performance of your code and
    prevent performance issues when dealing with large input sizes.
  </p>

  <p>
    For the more advanced readers, it is worth noting that the time complexity
    of the recursive Fibonacci implementation is technically O(φ<sup>n</sup>),
    where φ is the golden ratio (~1.618). However, to maintain the
    beginner-friendly nature of this tutorial, we will use the looser but still
    accurate bound of O(2<sup>n</sup>). See
    <a
      href="https://stackoverflow.com/questions/360748/computational-complexity-of-fibonacci-sequence"
      target="_blank"
      >here</a
    >
    if you are interested in a technical explanation.
  </p>

  <hr />
  <h4>O(n!)</h4>
  <p>
    O(n!) represents factorial time complexity, where the runtime of the
    algorithm grows as a factorial of the input size. This is one of the most
    inefficient time complexities, as it grows very quickly and is often
    impractical for larger inputs.
  </p>
  <p>
    An algorithm with O(n!) time complexity will execute n times for each
    element in the input, leading to a total of n! (n factorial) iterations. For
    example, an algorithm that generates all permutations of an input sequence
    would have a time complexity of O(n!), as the number of permutations of n
    elements is n!.
  </p>
  <p>
    Due to the exponential growth of O(n!), it's often only practical for small
    inputs. For larger inputs, the algorithm can take an extremely long time to
    run, making it unfeasible for most real-world applications. In general, it's
    best to avoid using algorithms with O(n!) time complexity unless absolutely
    necessary, and to explore more efficient alternatives whenever possible.
  </p>
  <hr />
  <h4>Good, Bad, and Ugly</h4>
  <p>
    By now, you should have a solid understanding of how to determine the time
    complexity of algorithms. However, you may still wonder what constitutes an
    exceptional time complexity. As a general rule of thumb, it's best to avoid
    any algorithm with a time complexity of O(n<sup>2</sup>) or worse. Although
    sometimes we may not be able to avoid using inefficient algorithms, it's
    crucial to be mindful of the size of inputs we provide. Our goal is to
    design algorithms with the best possible time complexity while still
    achieving the desired results.
  </p>
  <p>
    While it's ideal to avoid algorithms with a time complexity of
    O(n<sup>2</sup>) or worse, there are cases where it's unavoidable. In these
    situations, it's essential to be mindful of the size of your inputs and
    optimize your code for the best possible performance while still delivering
    the expected results. There are techniques that will be introduced in later
    post that can help optimize code and reduce its time complexity.
  </p>

  <hr />
  <h4>Conclusion</h4>
  <p>
    In conclusion, Big-O notation is an essential tool for analyzing the
    performance of algorithms. By using Big-O notation, we can estimate how an
    algorithm will behave as its input size grows, allowing us to identify
    bottlenecks and inefficiencies in our code. By understanding the various
    Big-O classifications and their implications, we can make informed decisions
    about which algorithms to use in different situations. While the
    mathematical theory behind Big-O notation can be complex, it is essential to
    have a basic understanding of it to be an effective software engineer. I
    hope this post has provided you with a clear understanding of Big-O notation
    and its significance in computer science, allowing you to write more
    efficient and optimized code. Remember, always analyze the time complexity
    of your algorithms, and use the appropriate algorithm for the task at hand.
  </p>
</div>
