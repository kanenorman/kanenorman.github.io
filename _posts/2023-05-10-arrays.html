---
layout: posts
title: "Arrays (Part I)"
author: Kane Norman
categories: [programming]
excerpt: Fixed-length sequential memory
order: 3
---

<div class="discussion-content">
  <h4>Introduction</h4>
  <p>
    Arrays are important data structures used in programming. They are usually
    one of the first things programmers learn, but many people don't fully grasp
    how they actually work. It's essential to understand how arrays function on
    a basic level in order to make the most of their capabilities and
    efficiency. Arrays are designed to take advantage of the way computer memory
    is organized, which allows them to be very efficient in storing and
    retrieving data.
  </p>
  <p>
    It's worth noting that some readers may be familiar with the Python list
    object. While it is often referred to as an array, this statement is
    technically incorrect. The Python list object is more specifically an
    implementation of a dynamic array, which we will discuss in future post. In
    this post, our focus will be on fixed-size arrays, particularly the
    <a
      href="https://docs.python.org/3/library/array.html#module-array"
      target="_blank"
      >array</a
    >
    object in the array module.
  </p>
  <hr />
  <h4>Computer Memory</h4>
  <p>
    To help better understand arrays, let's start by exploring how computer
    memory works in simple terms. When computers store information, they use
    tiny units called "bits." Think of bits as the building blocks of
    information, where each bit can represent either a 0 or a 1. Now, to make
    sense of these bits, computers group them into larger units called "bytes."
    A byte consists of 8 bits. In modern computer systems, the memory is
    organized in a way that allows us to access individual bytes easily. This
    concept is called "byte-addressable memory." In very simple terms, it
    basically means that each byte in the memory has a unique address associated
    with it. Think of it as a home address for each byte, enabling us to locate
    and retrieve specific information from the memory.
  </p>
  <figure>
    <p style="text-align: center; font-size: medium">
      ... | 2000 | 2001 | 2003 | 2004 | 2005 |2006 | ...
    </p>
    <figcaption
      style="text-align: center; font-size: small; font-style: italic"
    >
      Visualization of computer memory where each number represents the memory
      address of a specific byte
    </figcaption>
  </figure>

  <p>
    Now, when we work with objects in a program, such as variables or arrays,
    they take up a certain number of bytes in the memory. For instance, a 32-bit
    integer typically requires 4 bytes to store. This means that we can allocate
    4 consecutive bytes in the memory to hold that integer value.
  </p>
  <p>
    Applying this understanding to arrays, let's consider storing four 32-bit
    integers. We would need a total of 4 * 32 bits, which equals 128 bits or 16
    bytes of memory (Since each of the four integers requires 4 bytes,
    multiplying by 4 gives us the total memory requirement of 16 bytes).
    Allocating this amount of memory ensures sufficient space to accommodate all
    four integers within the array.
  </p>

  <hr />
  <h4>How are Arrays Directly Related to Computer Memory?</h4>
  <p>
    Arrays are a collection of sequential memory addresses. When we create an
    array, as shown in the following code snippet, we specify the data type and
    the values to be stored. In this example, we create an array of 32-bit
    integers with values 1, 2, 3, and 4. Under the hood, our computer allocates
    16 bytes of contiguous memory to store these values (4 bytes for each
    integer).
  </p>
  <pre>
{% highlight python %}
from array import array

my_array = array('i', [1,2,3,4])
{% endhighlight%}
</pre
  >
  <p>
    The significance of storing values in contiguous memory lies in the fact
    that it enables fast access to the values in an array. Given the address of
    the first element and the uniformity of each element's size in terms of
    bytes, finding the address of any element can be achieved through simple
    arithmetic operations, specifically, addition.
  </p>

  <p>
    Here is an example to make this clear. Consider array of integers, where
    each integer takes 4 bytes of memory. If the first integer is stored at
    address 1000, the second integer would be stored at address 1004 (since 1000
    + 4 = 1004), the third integer would be stored at address 1008, and so on.
    As you can see, the memory addresses of each integer are contiguous, meaning
    they are in a sequential order. This makes it easy for the computer to
    locate any element in the array by simply adding the size of one element (4
    bytes in this case) to the starting address.
  </p>
  <p>
    On the other hand, if the elements in the array were stored in
    non-contiguous memory, the computer would have to do more work to find the
    address of each element, which would slow down the access time.
  </p>
  <hr />
  <h4>Strengths and Weaknesses</h4>
  <p>
    Arrays offer several strengths that make them a popular choice for storing
    and accessing data. One of their key strengths lies in their ability to
    provide speedy access to elements. This is achieved through contiguous
    memory allocation, allowing the computer to quickly locate any element in
    the array by performing a simple calculation using the size of one element
    and the starting address.
  </p>
  <p>
    While access is speedy, arrays have speed limitations when it comes to
    insertion, deletion, searching, and sorting operations. Unlike specialized
    data structures designed for these tasks, arrays may exhibit slower and less
    efficient performance. These operations often require shifting or
    reallocation of elements, which can result in additional overhead and impact
    the overall efficiency of the array.
  </p>

  <hr />
  <h4>Big-O Analysis</h4>

  <div class="table-wrapper">
    <table class="alt">
      <thead>
        <tr>
          <th>Operation</th>
          <th>Time Complexity</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Access</td>
          <td>O(1)</td>
        </tr>
        <tr>
          <td>Insert</td>
          <td>O(n)</td>
        </tr>
        <tr>
          <td>Search</td>
          <td>O(n)</td>
        </tr>
        <tr>
          <td>Delete</td>
          <td>O(n)</td>
        </tr>
      </tbody>
    </table>
  </div>
  <p>
    Accessing elements in an array can be achieved in a constant amount of time,
    regardless of the size of the array. This is due to the fact that the
    elements are stored in a contiguous fashion, making it possible to access
    any element quickly. To access a specific element, we simply take the base
    address and add the required number of bytes. This results in a time
    complexity of O(1), meaning constant time.
  </p>
  <p>
    When it comes to inserting elements into an array, the speed of the
    operation depends on the position where the insertion needs to occur. Adding
    elements at the end of an array is fast and straightforward. We can simply
    calculate the appropriate number of bytes to add to the base address and
    insert the new value. This operation has a constant time complexity of O(1).
  </p>
  <p>
    However, inserting elements at the beginning or in the middle of an array
    can be slow and inefficient, especially in the worst-case scenario. The time
    complexity for such operations is O(n), where n represents the number of
    elements in the array. This slow performance arises because inserting an
    element in the middle requires shifting a significant portion of the
    existing elements to create space for the new element. With n elements in
    the array, it may be necessary to move all n elements, resulting in a linear
    time complexity of O(n).
  </p>
  <p>
    Searching for an element in an array often requires using a linear search
    algorithm, which involves iterating over each element in the array until the
    desired value is found. In the best-case scenario, the element being
    searched for is located at the beginning of the array, resulting in an
    almost immediate termination of the algorithm. However, it's important to
    consider the worst-case scenario, where the element is either located at the
    end of the array or not present at all. In such cases, we would need to
    iterate through most, if not all, of the n elements in the array. As a
    result, the worst-case time complexity for the linear search algorithm is
    O(n).
  </p>
  <p>
    Deleting elements from arrays also has a worst-case time complexity of O(n)
    for similar reasons to insertion. After removing an element, the remaining
    elements must be shifted to close the gap left by the deletion, potentially
    requiring the movement of all n elements.
  </p>
  <hr />
  <h4>Conclusion</h4>
  <p>
    Arrays are simple and highly effective data structures. They are especially
    fast for accessing data, but rather slow for insertion, deletion, searching
    and sorting. In the upcoming post, we will explore other data structures
    more suited for these task, and discuss how to sort elements in arrays. In
    the next, post, we will continue our discussion of arrays by introducing
    dynamic arrays which are similar but do not have a fixed size.
  </p>
</div>
